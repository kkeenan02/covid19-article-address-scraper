{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 emerging reseaerch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from re import search\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"COVID19 - COVID-19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(URL):\n",
    "    \"\"\"\n",
    "    get the page of a given journal article\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(URL, headers = headers)\n",
    "        if resp.status_code == 200:\n",
    "            return bs(resp.text)\n",
    "        else:\n",
    "            print(resp.status_code)\n",
    "    except:\n",
    "        print(\"Page retrieval failed\")\n",
    "\n",
    "def get_author_address(soup):\n",
    "    \"\"\"\n",
    "    parse first author address from paper metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        preout = soup.findAll(\"meta\", {\"name\": \"citation_author_institution\"})\n",
    "        for x in preout:\n",
    "            if len(x.get(\"content\")) != 0:\n",
    "                out = x.get(\"content\")\n",
    "                break\n",
    "        return out\n",
    "    except:\n",
    "        try:\n",
    "            out = soup.find(\"p\", {\"class\": \"affiliation\"}).text\n",
    "            return out\n",
    "        except:\n",
    "            try:\n",
    "                out = soup.find(\"div\", {\"class\": \"accordion article__author-affiliations\"}).text\n",
    "                return out\n",
    "            except:\n",
    "                try:\n",
    "                    out = soup.find(\"span\", {\"class\": \"aff-overlay\"}).findAll(\"li\")[1].text\n",
    "                    return out\n",
    "                except:\n",
    "                    try:\n",
    "                        out = soup.find(\"span\", {\"class\": \"affiliation\"}).text\n",
    "                        return out\n",
    "                    except:\n",
    "                        try:\n",
    "                            out = soup.find(\"div\", {\"class\": \"artice-info-affiliation\"}).text\n",
    "                            return out\n",
    "                        except:\n",
    "                            try:\n",
    "                                out = soup.find(\"div\", {\"class\": \"author-info\"}).text\n",
    "                                return out\n",
    "                            except:\n",
    "                                try:\n",
    "                                    out = soup.find(\"div\", {\"class\": \"loa-info-affiliations-info\"}).text\n",
    "                                    return out\n",
    "                                except:\n",
    "                                    try:\n",
    "                                        out = soup.find(\"div\", {\"class\": \"affiliation-name\"}).text\n",
    "                                        return out\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            preout = soup.findAll(\"p\", {\"class\": \"f-body\"})\n",
    "                                            for x in preout:\n",
    "                                                if x.find(\"a\", {\"class\": \"email\"}):\n",
    "                                                    out = x.text\n",
    "                                                    break\n",
    "                                            return out\n",
    "                                        except:\n",
    "                                            try:\n",
    "                                                out = soup.find(\"section\", {\"id\": \"author_affiliations\"}).find(\"p\", {\"class\": \"f-body--sm\"}).text\n",
    "                                                return out\n",
    "                                            except:\n",
    "                                                try:\n",
    "                                                    out = soup.find(\"td\", {\"class\": \"PubAff\"}).text\n",
    "                                                    return out\n",
    "                                                except:\n",
    "                                                    try:\n",
    "                                                        out = soup.find(\"div\", {\"class\": \"origin_section02\"}).findAll(\"p\")[1].text\n",
    "                                                        return out\n",
    "                                                    except:\n",
    "                                                        try:\n",
    "                                                            out = soup.find(\"div\", {\"class\": \"article-inner_html\"}).findAll(\"p\")\n",
    "                                                            return out\n",
    "                                                        except:\n",
    "                                                            try:\n",
    "                                                                out = soup.find(\"a\", {\"class\": \"entryAuthor\"}).find(\"span\", {\"class\": \"overlay\"}).text\n",
    "                                                                return out\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    out = soup.find(\"div\", {\"class\": \"aff\"}).text\n",
    "                                                                    return out\n",
    "                                                                except:\n",
    "                                                                    try:\n",
    "                                                                        out = soup.find(\"span\", {\"id\": re.compile(r\"affilia\\d+\")}).text\n",
    "                                                                        return out\n",
    "                                                                    except:\n",
    "                                                                        try:\n",
    "                                                                            out = soup.find(\"p\", {\"id\": re.compile(r\"AF_ID.*?\")}).text\n",
    "                                                                            return out\n",
    "                                                                        except:\n",
    "                                                                            try:\n",
    "                                                                                out = soup.find(\"span\", {\"class\": \"affiliations\"}).text\n",
    "                                                                                return out\n",
    "                                                                            except:\n",
    "                                                                                try:\n",
    "                                                                                    out = soup.find(\"dl\", {\"id\": \"afflist1\"}).find(\"dt\").text\n",
    "                                                                                    return out\n",
    "                                                                                except:\n",
    "                                                                                    print(\"Address not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_address_data = data[data[\"Full Address\"].str.contains(\"^\\s+$\")]\n",
    "urls = no_address_data[\"URL\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_url = [x[\"url\"] for x in pages]\n",
    "for url in urls:\n",
    "    if url is not np.nan and url not in done_url:\n",
    "        print(url)\n",
    "        soup = get_page(url)\n",
    "        pages.append(\n",
    "            {\n",
    "                \"url\": url,\n",
    "                \"page\": soup\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    author_address = get_author_address(page[\"page\"])\n",
    "    addresses.append(\n",
    "        {\n",
    "            \"url\": page[\"url\"],\n",
    "            \"full_address\": author_address\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = [x[\"url\"] for x in addresses if not x[\"full_address\"]]\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = [\"http://dx.doi.org/10.33321/cdi.2020.44.21\", \"http://dx.doi.org/10.4062/biomolther.2019.202\", \"http://dx.doi.org/10.3346/jkms.2020.35.e79\", \"http://dx.doi.org/10.1001/jama.2020.2633\", \"http://dx.doi.org/10.3348/kjr.2020.0112\", \"http://dx.doi.org/10.1056/NEJMe2004244\", \"http://dx.doi.org/10.2471/blt.20.251561\", \"http://dx.doi.org/10.26355/eurrev_202002_20396\", \"http://dx.doi.org/10.24875/RIC.20000023\", \"http://dx.doi.org/10.1021/cen-09809-leadcon\", \"http://dx.doi.org/10.1136/vr.m1009\", \"http://dx.doi.org/10.1111/ijcp.13365\", \"http://dx.doi.org/10.1002/adaw.32661\", \"http://dx.doi.org/10.1017/S1049023X2000031X\", \"http://dx.doi.org/10.21037/apm.2020.02.33\", \"http://dx.doi.org/10.1056/NEJMe2004856\", \"http://dx.doi.org/10.1701/3315.32860\", \"http://dx.doi.org/10.26434/CHEMRXIV.11902623.V2\", \"http://dx.doi.org/10.1111/tan.13841\"]\n",
    "unparsed_pages = [x for x in pages if x[\"url\"] in missing and x[\"page\"] and \"ipAddress : '188.29.188.131'\" not in x[\"page\"].get_text() and not search(r\"\\/science\\.\", x[\"url\"]) and \"Thank you for visiting nature.com\" not in x[\"page\"].get_text() and not x[\"page\"].find(\"a\", {\"href\": \"mailto:NewMedia@cma.org.cn\"}) and x[\"url\"] not in excluded and \"CHEMRXIV\" not in x[\"url\"] and not search(r\"\\/cen\\-\\d+\\-\", x[\"url\"]) and not search(r\"\\/vr\\.[a-z0-9]+$\", x[\"url\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unparsed_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unparsed_pages[0][\"page\"]#.find(\"dl\", {\"id\": \"afflist1\"}).find(\"dt\").text\n",
    "# <span id=\"sp_affiliation\">  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unparsed_pages[0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_addr = [x for x in addresses if x[\"full_address\"]]\n",
    "len(parsed_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(parsed_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"article_addresses.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
